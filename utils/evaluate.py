import os, pdb, sys
import re
import json
import torch
import numpy as np
import pandas as pd
import random

from torch import nonzero
from numpy.linalg import norm
from lexical_diversity import lex_div
from tqdm import tqdm as progress_bar
from collections import Counter, defaultdict
from sklearn.metrics import accuracy_score
from assets.static_vars import device, debug_break
# metric = load_metric('bleu')  'bertscore', ''  
# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from mwzeval.metrics import Evaluator

def parse_output(generated_string, task):
  """accepts the full string generated by the model and 
  returns a structured representation for comparison with the target"""
  parsed = defaultdict(list)
  # context_str = generated_string.split('<label>')[0]
  pred_string = generated_string.split('<label>')[1]
  pred_string = pred_string.replace(' <pad>', '') 

  if '<' in pred_string:  # represents the start of a special token
    eos_index = pred_string.index('<')
  else:
    return parsed  # we found nothing
  pred_string = pred_string[:eos_index].strip()

  if task == 'sgd':
    return parse_sgd(pred_string, parsed)
  elif task == 'tt':
    return parse_tt(pred_string, parsed)
  elif task == 'mwoz':
    return parse_mwoz(pred_string, parsed)
  elif task == 'delex':
    return pred_string

def parse_sgd(pred_string, parsed):
  for pred in pred_string.split(';'):
    try:
      remaining, value = pred[:-1].split("=")
      intent, slot = remaining.split("(")
    except(ValueError):
      continue

    if slot == 'request':
      parsed['requests'].append(value)
    else:
      parsed['slots'].append(slot)
      parsed['values'].append(value)
    parsed['intents'].append(intent)

  return parsed

def parse_mwoz(pred_string, parsed):
  domain_preds = pred_string.split("<sep>") # and the

  for dpred in domain_preds:
    dpred = dpred.strip()
    try:
      position = dpred.index(":")  # state
      domain = dpred[:position].strip()
      prediction = dpred[position+1:]
    except(ValueError):
      continue

    for pred in prediction.strip().split(';'):
      for swap in swaps:
        if swap in pred:
          replacement = swaps[swap]
          pred = pred.replace(swap, replacement)

      parts = pred.split()
      if len(parts) < 2:
        continue
      elif len(parts) == 2:
        slot, val = parts
      else:
        slot = parts[0]
        val = ' '.join(parts[1:])

      if val != 'none':
        parsed[domain].append((slot, val))

  return parsed

def calculate_prec_rec(predicted_outputs, labels):
  label_keys = ['intents', 'requests', 'slots', 'values']  # services is part of input
  match, possible_matches = 0, 0.001
  found, possible_found = 0, 0.001

  for pred_string, label_dict in zip(predicted_outputs, labels):
    parsed = parse_pred_output(pred_string, label_keys)

    for key in label_keys:
      predicted_items = parsed[key]
      target_items = label_dict[key]

      for pi in predicted_items:
        if pi in target_items:
          match += 1
        possible_matches += 1

      for ti in target_items:
        if ti in predicted_items:
          found += 1
        possible_found += 1

  precision = round(match / possible_matches, 3)
  recall = round(found / possible_found, 3)
  return precision, recall

def calculate_mwoz_2_2(predictions, targets):
  # generated_convos must be a dictionary where the key is the convo_id
  # the value of the prediction should be a list of dicts, where each dict is a response
  evaluator = Evaluator(bleu=True, success=False, richness=False, dst=True)

  convos = defaultdict(list)
  for pred, dialog_id in zip(predictions, dialog_ids):
    convo_id, turn_count, domains_string = dialog_id.split('_')
    active_domains = domains_string.split(';')

    parsed_pred = parse_output(pred, task='mwoz')
    pred_list = parsed_pred if isinstance(parsed_pred, list) else []
    turn_tuple = (pred_list, int(turn_count), active_domains)
    convos[convo_id].append(turn_tuple)

  generated_convos = {}
  for convo_id, turn_tuples in convos.items():

    convo_pred = []
    for turn_tuple in turn_tuples:
      turn_tuple.sort(key=lambda x: x[1])  # sort by turn_count

      for pred_tup in turn_tuple:
        turn_pred = {"state": set(pred_tup[0]), 'active_domains': pred_tup[2]}
        convo_pred.append(turn_pred)
    generated_convos[convo_id] = convo_pred

  results = evaluator.evaluate(generated_convos)
  return results


def eval_quantify(args, predictions, targets, exp_logger, tokenizer, split):
  results = {'epoch': exp_logger.epoch, 'loss': round(exp_logger.eval_loss, 3) }

  if args.dataset == 'mwoz':
    results = calculate_mwoz_2_2(predictions, targets, evaluator)
    bleu_score = mwoz_res['bleu']['mwz22']
    inform_rate = mwoz_res['success']['inform']['total']
    success_rate = mwoz_res['success']['success']['total']
    results['bleu'] = round(bleu_score, 2)
    results['inform'] = inform_rate
    results['success'] = success_rate
    results['combined'] = round(((inform_rate + success_rate) / 2 ) + bleu_score, 3)


  
  exp_logger.log_info(results)
  return results

def clean_text(input_text, tokenizer):
  input_text = input_text.replace(tokenizer.pad_token, '')
  input_text = input_text.replace(tokenizer.bos_token, '')
  input_text = input_text.replace(tokenizer.eos_token, '')
  cleaned = input_text.split(" <")
  return cleaned

def eval_qualify(args, predictions, targets, contexts, exp_logger, tokenizer):
  preds = torch.argmax(predictions, axis=1) 
  
  results = []
  for pred, target, context in zip(preds, targets, contexts):
    input_text = tokenizer.decode(context) # , skip_special_tokens=True)
    cleaned_text = clean_text(input_text, tokenizer)
    target_text = exp_logger.ontology[target]
    pred_text = exp_logger.ontology[pred]

    if pred_text != target_text:
      if args.do_save:
        res = ' '.join(cleaned_text) + '---' + pred_text + '---' + target_text + '\n'
        results.append(res)
      else:
        for line in cleaned_text:
          print(line)
        print('predicted:', pred_text, ', actual:', target_text)
        pdb.set_trace()

  if args.do_save:
    save_filepath = os.path.join(exp_logger.save_path, 'qualify.txt')
    with open(save_filepath, 'w') as file:
      file.writelines(results)
    print(len(results), "results written to", save_filepath)
  return results

def dst_breakdown(predictions, labels, results):
  label_keys = ['intents', 'requests', 'slots', 'values']

  matches = defaultdict(float)
  poss_match = defaultdict(float)
  founds = defaultdict(float)
  poss_found = defaultdict(float)

  for prediction, label_dict in zip(predictions, labels):
    parsed = parse_pred_output(prediction, label_keys)

    for key in label_keys:
      predicted_items = parsed[key]
      target_items = label_dict[key]

      for pi in predicted_items:
        if pi in target_items:
          matches[key] += 1
        poss_match[key] += 1

      for ti in target_items:
        if ti in predicted_items:
          founds[key] += 1
        poss_found[key] += 1

      # to avoid divide by zero
      matches[key] += 0.001
      poss_match[key] += 0.001
      founds[key] += 0.001
      poss_found[key] += 0.001

  aggregate_match, aggregate_poss_match = 0, 0  
  aggregate_found, aggregate_poss_found = 0, 0

  for lk in label_keys:
    match = matches[lk]
    possible_matches = poss_match[lk]
    found = founds[lk]
    possible_found = poss_found[lk]

    aggregate_match += match
    aggregate_poss_match += possible_matches
    aggregate_found += found
    aggregate_poss_found += possible_found

    result[f'{lk}_prec'] = round(match / possible_matches, 3)
    result[f'{lk}_rec'] = round(found / possible_found, 3)

  results['precision'] = round(aggregate_match / aggregate_poss_match, 3)
  results['recall'] = round(aggregate_found / aggregate_poss_found, 3)
  return results

if __name__ == '__main__':
  args = solicit_params()
  args.multiwoz_version = '2.1'
  args.use_action = True
  args.use_knowledge = True

  random.seed(14)
  np.random.seed(14)
  # joint_acc, _ = eval_dst(args)
  joint_acc, _ = eval_confidence(args)
  print('joint accuracy: {}%'.format(joint_acc))

