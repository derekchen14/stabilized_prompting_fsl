import os, pdb, sys
import re
import json
import torch
import numpy as np
import pandas as pd
import random

from torch import nonzero
from numpy.linalg import norm
# from lexical_diversity import lex_div
from tqdm import tqdm as progress_bar
from collections import Counter, defaultdict
from sklearn.metrics import accuracy_score
from assets.static_vars import device, debug_break
# metric = load_metric('bleu')  'bertscore', ''  
# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
# from mwzeval.metrics import Evaluator

def parse_output(generated_string, model_type):
  """accepts the full string generated by the model and 
  returns a parsed string for comparison with the target"""
  if model_type in ['bart', 't5']:
    pred_string = generated_string[7:]
  elif model_type == 'gpt':
    try:
      pred_string = generated_string.split('<label>')[1]
    except(IndexError):
      print("unable to split by label")
      pred_string = generated_string
    pred_string = pred_string.replace(' <pad>', '')
    
  if '<' in pred_string:  # represents the start of a special token
    eos_index = pred_string.index('<')
  else:
    return '' # we found nothing
  pred_string = pred_string[:eos_index]
  parsed_str = normalize_text(pred_string)
  return parsed_str 

def parse_sgd(pred_string, parsed):
  for pred in pred_string.split(';'):
    try:
      remaining, value = pred[:-1].split("=")
      intent, slot = remaining.split("(")
    except(ValueError):
      continue

    if slot == 'request':
      parsed['requests'].append(value)
    else:
      parsed['slots'].append(slot)
      parsed['values'].append(value)
    parsed['intents'].append(intent)

  return parsed

def parse_mwoz(pred_string, parsed):
  domain_preds = pred_string.split("<sep>") # and the

  for dpred in domain_preds:
    dpred = dpred.strip()
    try:
      position = dpred.index(":")  # state
      domain = dpred[:position].strip()
      prediction = dpred[position+1:]
    except(ValueError):
      continue

    for pred in prediction.strip().split(';'):
      for swap in swaps:
        if swap in pred:
          replacement = swaps[swap]
          pred = pred.replace(swap, replacement)

      parts = pred.split()
      if len(parts) < 2:
        continue
      elif len(parts) == 2:
        slot, val = parts
      else:
        slot = parts[0]
        val = ' '.join(parts[1:])

      if val != 'none':
        parsed[domain].append((slot, val))

  return parsed

def calculate_prec_rec(predicted_outputs, labels):
  label_keys = ['intents', 'requests', 'slots', 'values']  # services is part of input
  match, possible_matches = 0, 0.001
  found, possible_found = 0, 0.001

  for pred_string, label_dict in zip(predicted_outputs, labels):
    parsed = parse_pred_output(pred_string, label_keys)

    for key in label_keys:
      predicted_items = parsed[key]
      target_items = label_dict[key]

      for pi in predicted_items:
        if pi in target_items:
          match += 1
        possible_matches += 1

      for ti in target_items:
        if ti in predicted_items:
          found += 1
        possible_found += 1

  precision = round(match / possible_matches, 3)
  recall = round(found / possible_found, 3)
  return precision, recall


re_art = re.compile(r'\b(a|an|the)\b')
re_punc = re.compile(r'[!"#$%&()*+,-./:;<=>?@\[\]\\^`{|}~_\']')

def normalize_text(s):
  # Lower text and remove punctuation, articles and extra whitespace.
  s = s.lower().strip()
  s = re_punc.sub(' ', s)
  s = re_art.sub(' ', s)
  s = ' '.join(s.split())
  return s

def group_by_convo(predictions, targets):
  """groups all predictions by conversation, parses the output, and then sorts by turn
  predictions: list of the raw string out from the model
  targets: list of targets extracted from examples
  """
  convos = defaultdict(dict)
  for pred, target in zip(predictions, targets):
    convo_id, turn_count = target['global_id'].split('_')
    parsed = parse_output(pred)
    turn_tuple = (parsed, target['slot'], target['value'])
    if turn_count not in convos[convo_id]:
      convos[convo_id][turn_count] = []
    convos[convo_id][turn_count].append(turn_tuple)
  
  for convo_id, turns in convos.items():
    turns.sort(key=turn_count)
  return convos

def pred_to_dialog_state(grouped_preds):
  generated_convos = {}
  for convo_id, convo_turns in grouped_preds.items():
    # convo turns is a dictionary with keys of turn_count and values of turn_tuples
    max_turn = max(list(convo_turns.keys()))   # largest turn count in the convo
    generated_turns = order_by_turn_group_by_ds(convo_turns, max_turn+1)
    generated_convos[convo_id] = generated_turns
  return generated_convos

def order_by_turn_group_by_ds(convo_turns, max_turns):
  generated_turns = []

  for turn_index in range(max_turns):
    if turn_index in convo_turns:
      turn_tuples = convo_turns[turn_index]
      gen_turn = {'state': defaultdict(dict), 'active_domains': turn_tuples[0][2]}
      
      # Group by domain
      response_tokens = []
      for pred_value, dsv, _ in turn_tuples:
        domain, slot, target_value = dsv
        gen_turn['state'][domain][slot] = pred_value
        response_tokens.extend([domain, slot, target_value])
      gen_turn['response'] = ' '.join(response_tokens)  # filler to satisfy evaluator
      generated_turns.append(gen_turn)  # in order due to looping by turn_count
    
  return generated_turns

def calculate_jga(results, final_preds):
  """ Does not currently calculate JGA, just gives a sketch 
  should return a results dictionary that contains the JGA and anything else you want to log
  """
  possible, correct = 0, 0
  joint_possible, joint_correct = 0, 0

  for convo_id, convo_turns in grouped_preds.items():
    
    turn_correct = True
    for turn_count, turn_tuple in convo_turns.items():
      pred, slot, value = turn_tuple
      if value != '<none>':
        if pred == value:
          correct += 1
        else:
          turn_correct = False
        possible += 1

    if turn_correct:
      joint_correct += 1
    joint_possible += 1

  results['accuracy'] = round(float(correct) / possible, 3)
  results['jga'] = round(float(joint_correct) / joint_possible, 3)
  return results

def fill_carryover(args, preds):
  for convo_id, convo_turns in progress_bar(preds.items(), total=len(preds)):

    predicted_values = {}
    for turn_count, turn_tuple in convo_turns.items():
      pred, slot, value = turn_tuple

      if pred == '<none>':
        pred = predicted_values[slot] # then carry over the old value
      elif pred == '<remove>':
        pred = '<none>'

      convo_turns[turn_count] = (pred, slot, value)
      predicted_values[slot] = pred  # store it for the next round

  return preds

def eval_quantify(args, predictions, targets, exp_logger, tokenizer, split):
  results = {'epoch': exp_logger.epoch }  # 'loss': exp_logger.eval_loss  (no loss by default)

  if args.style == 'dataset':
    # the left out query set is MWOZ or SGD
    grouped_preds = group_by_convo(predictions, targets)
    final_preds = fill_carryover(args, grouped_preds)
    results = calculate_jga(results, final_preds)

  elif args.style == 'domain':
    # the left out query set is hotel, attraction, taxi, etc.
    pass

  exp_logger.log_info(results)
  return results


def eval_qualify(args, predictions, targets, contexts, exp_logger, tokenizer):
  preds = torch.argmax(predictions, axis=1) 
  
  results = []
  for pred, target, context in zip(preds, targets, contexts):
    input_text = tokenizer.decode(context) # , skip_special_tokens=True)
    cleaned_text = clean_text(input_text, tokenizer)
    target_text = exp_logger.ontology[target]
    pred_text = exp_logger.ontology[pred]

    if pred_text != target_text:
      if args.do_save:
        res = ' '.join(cleaned_text) + '---' + pred_text + '---' + target_text + '\n'
        results.append(res)
      else:
        for line in cleaned_text:
          print(line)
        print('predicted:', pred_text, ', actual:', target_text)
        pdb.set_trace()

  if args.do_save:
    save_filepath = os.path.join(exp_logger.save_path, 'qualify.txt')
    with open(save_filepath, 'w') as file:
      file.writelines(results)
    print(len(results), "results written to", save_filepath)
  return results

def dst_breakdown(predictions, labels, results):
  label_keys = ['intents', 'requests', 'slots', 'values']

  matches = defaultdict(float)
  poss_match = defaultdict(float)
  founds = defaultdict(float)
  poss_found = defaultdict(float)

  for prediction, label_dict in zip(predictions, labels):
    parsed = parse_pred_output(prediction, label_keys)

    for key in label_keys:
      predicted_items = parsed[key]
      target_items = label_dict[key]

      for pi in predicted_items:
        if pi in target_items:
          matches[key] += 1
        poss_match[key] += 1

      for ti in target_items:
        if ti in predicted_items:
          founds[key] += 1
        poss_found[key] += 1

      # to avoid divide by zero
      matches[key] += 0.001
      poss_match[key] += 0.001
      founds[key] += 0.001
      poss_found[key] += 0.001

  aggregate_match, aggregate_poss_match = 0, 0  
  aggregate_found, aggregate_poss_found = 0, 0

  for lk in label_keys:
    match = matches[lk]
    possible_matches = poss_match[lk]
    found = founds[lk]
    possible_found = poss_found[lk]

    aggregate_match += match
    aggregate_poss_match += possible_matches
    aggregate_found += found
    aggregate_poss_found += possible_found

    result[f'{lk}_prec'] = round(match / possible_matches, 3)
    result[f'{lk}_rec'] = round(found / possible_found, 3)

  results['precision'] = round(aggregate_match / aggregate_poss_match, 3)
  results['recall'] = round(aggregate_found / aggregate_poss_found, 3)
  return results

if __name__ == '__main__':
  args = solicit_params()
  args.multiwoz_version = '2.1'
  args.use_action = True
  args.use_knowledge = True

  random.seed(14)
  np.random.seed(14)
  # joint_acc, _ = eval_dst(args)
  joint_acc, _ = eval_confidence(args)
  print('joint accuracy: {}%'.format(joint_acc))

